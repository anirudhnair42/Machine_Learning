{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CS156_Assignment_3.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHV5IurrxKWH"
      },
      "source": [
        "# Assignment 3\n",
        "## CS156 | Prof. Sterne\n",
        "### Anirudh Nair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKkVAAo0xKWL"
      },
      "source": [
        "Colab Link:https://colab.research.google.com/drive/1XFkRxotPD2ualhUua2GlKQBp3byqrnMg#scrollTo=rh7RAWpHROxi (higher sample size for testing and training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JrZkfQLxKWN"
      },
      "source": [
        "#importing the required libraries\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import PIL.ImageOps\n",
        "import random as random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CItvsSlxKWT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "6ce47f34-0a46-4a93-b207-6cb279a35d3f"
      },
      "source": [
        "#unzipping the downloaded file and loading the jersey and shirt folders into their respective datasets\n",
        "with ZipFile('jerseys (1).zip') as zipObj: zipObj.extractall()\n",
        "with ZipFile('shirts (1).zip') as zipObj: zipObj.extractall()\n",
        "size = (200,250)\n",
        "jersey = glob(\"Jersey - n03595614/*\")\n",
        "shirt = glob(\"Shirt - n04197391/*\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9d00a01443b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jerseys (1).zip'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shirts (1).zip'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjersey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Jersey - n03595614/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshirt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shirt - n04197391/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jerseys (1).zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPncYuMOxKWW"
      },
      "source": [
        "#Standardizing all the uploaded images into one size\n",
        "standardize = (200,250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEN_KoZWxKWZ"
      },
      "source": [
        "# This function will be used to conver the uploaded files into a numpy array of RGB pixels\n",
        "def img_to_array(filename):\n",
        "    img = Image.open(filename)\n",
        "    img = img.resize(standardize)\n",
        "    img = list(img.getdata())\n",
        "    img = np.array(img)\n",
        "    img_wide = img.reshape(1, -1)\n",
        "    return img_wide[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7nLg1r1xKWc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "88cad5e0-6c8a-4343-f7e3-d8f98b428f5a"
      },
      "source": [
        "#creating the combined dataset\n",
        "combined = []\n",
        "\n",
        "for i in jersey[0:1000]:\n",
        "    combined.append([img_to_array(i), \"jersey\"])\n",
        "    \n",
        "for i in shirt[0:1000]:\n",
        "    combined.append([img_to_array(i), \"shirt\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dcabc13da751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjersey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jersey\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'jersey' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nda3U0sExKWg"
      },
      "source": [
        "#shuffling the shirts and jerseys in the combined dataset\n",
        "random.seed(100)\n",
        "random.shuffle(combined)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hur72iIuxKWj"
      },
      "source": [
        "#splitting the data into the training and testing sets\n",
        "train = combined[:1600] \n",
        "test = combined[1600:2000]\n",
        "x_train = np.array([cd for (cd,_y) in train], dtype=object) \n",
        "y_train = np.array([_y for (cd,_y) in train], dtype=object)\n",
        "x_test = np.array([cd for (cd,_y) in test])\n",
        "y_test = np.array([_y for (cd,_y) in test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXvbdyYZxKWm"
      },
      "source": [
        "import pandas as pd  \n",
        "x_train = x_train.tolist()\n",
        "y_train = y_train.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3utpDzBxKWp",
        "outputId": "7d569eec-5652-40f7-d606-93c716eb17ff"
      },
      "source": [
        "#building the classifier for the data\n",
        "model = LogisticRegression(solver = \"saga\")\n",
        "model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/anirudhnair/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BphSc9XhxKWu",
        "outputId": "bcd97055-3ba7-48f8-8684-51e2ac6b0746"
      },
      "source": [
        "ytrain_pred = model.predict(x_train)\n",
        "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(model.score(x_train, y_train)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of logistic regression classifier on train set: 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gns5rF7xKWy",
        "outputId": "4c64fa8e-85ee-48ab-d1a5-f2ce578b94d1"
      },
      "source": [
        "ytest_pred = model.predict(x_test)\n",
        "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(model.score(x_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of logistic regression classifier on test set: 0.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlF6-3fIxKW1"
      },
      "source": [
        "We first build a general classifier that uses logistic regression to classify the data into either a shirt or a jersey. The accuracy score on the training_set comes out to be .97 which makes sense as the model was trained using that data and there would always be some noise present in the data. However, the model does not perform so well on the training set given that it only achieves a accuracy score of .57. This shows us that weighing all elements of the image equally and using them to classify a clothing image as a shirt or jersey is not the best way. We need to focus down on some particular elements and then our accuracy might get better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0X8IOFpxKW2",
        "outputId": "0010059e-eaa5-43b8-ade4-c86d0156d674"
      },
      "source": [
        "pca = PCA(n_components=7)\n",
        "com_model = LogisticRegression(solver=\"saga\")\n",
        "\n",
        "pipe = Pipeline([('pca', pca), ('logistic', com_model)])\n",
        "\n",
        "\n",
        "print('model accuracy on training set after pca: %s'% str(sum(pipe.fit(x_train, y_train).predict(x_train) == y_train)/len(y_train)))\n",
        "print('model accuracy on test set after pca: %s'% str(sum(pipe.fit(x_train, y_train).predict(x_test)== y_test)/len(y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy on training set after pca: 0.6375\n",
            "model accuracy on test set after pca: 0.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaUlMuVjxKW5"
      },
      "source": [
        "However, when we run PCA, we see that the model does not even perform well on the training set, which was used to build the model. This could be explained by understanding the nature of PCA. PCA focuses on reducing dimensionality (getting rid of the unhelpful elements of the image) by focusing on the dimensions(elements) with maximum variation. This may not be a useful method to weigh and prioritize dimensions in ouur case because  shirt and jerseys could be similar in a lot of aspects such as size, which could also have the maximum variation in the data. Thus, the model proves its inefficiency by scoring a meagre 0.45 on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqAa_XmhxKW6",
        "outputId": "a937a1e9-5134-4109-976d-3077153b6721"
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=7)\n",
        "pipe2 = Pipeline([('lda', lda), ('logistic', com_model)])\n",
        "\n",
        "\n",
        "print('model accuracy on training set after lda: %s'% str(sum(pipe2.fit(x_train, y_train).predict(x_train) == y_train)/len(y_train)))\n",
        "print('model accuracy on test set after lda: %s'% str(sum(pipe2.fit(x_train, y_train).predict(x_test)== y_test)/len(y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/anirudhnair/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(150000, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/Users/anirudhnair/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n",
            "/Users/anirudhnair/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
            "  warnings.warn(\"Variables are collinear.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model accuracy on training set after lda: 0.9875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/anirudhnair/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(150000, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/Users/anirudhnair/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n",
            "/Users/anirudhnair/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
            "  warnings.warn(\"Variables are collinear.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model accuracy on test set after lda: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BHzl5JlxKW9"
      },
      "source": [
        "In the case of LDA, we see that the model achieves an accuracy of 0.9875 on the training set, which is higher than the accuracy that the general model had in the beginning. This could also be explained by the nature of LDA, as LDA chooses to reduce dimensionality by maximizing the seperability between different elements of the image. Thus, it will weigh elements that are different for shirts and jerseys more and priotitize them. However, when it comes to the test set, this model doesn't perform that well. This could be becuase the training set may be overfitting the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6GB9eoZxKW-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}